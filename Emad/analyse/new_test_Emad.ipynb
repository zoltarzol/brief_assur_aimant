{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer, make_column_transformer\n",
    "# sklearn.compose: The sklearn.compose module is a submodule of the sklearn library for machine learning in Python. It provides functions for creating complex preprocessing and modeling pipelines.\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler,PolynomialFeatures,RobustScaler\n",
    "#sklearn.preprocessing: The sklearn.preprocessing module is a submodule of the sklearn library that provides functions for preprocessing data, such as scaling and normalizing features, imputing missing values, and encoding categorical variables.\n",
    "from sklearn.linear_model import Ridge,LinearRegression,Lasso\n",
    "# sklearn.linear_model: The sklearn.linear_model module is a submodule of the sklearn library that provides functions for fitting linear models for regression and classification.\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# sklearn.pipeline: The sklearn.pipeline module is a submodule of the sklearn library that provides functions for creating and working with pipelines of transformers and models.\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,learning_curve, RandomizedSearchCV, cross_val_score, KFold\n",
    "# sklearn.model_selection: The sklearn.model_selection module is a submodule of the sklearn library that provides functions for splitting data into training and test sets, evaluating models using cross-validation, and hyperparameter tuning.\n",
    "from sklearn.dummy import DummyRegressor\n",
    "# sklearn.dummy: The sklearn.dummy module is a submodule of the sklearn library that provides simple dummy models for regression and classification.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement et affichage des donn√©es\n",
    "data = pd.read_csv('../data.csv')\n",
    "def classify_bmi(row):\n",
    "    if row[\"bmi\"] < 25:\n",
    "        return \"normal\"\n",
    "    elif row[\"bmi\"] < 30:\n",
    "        return \"overweight\"\n",
    "    else:\n",
    "        return \"obese\"\n",
    "\n",
    "data[\"bmi_class\"] = data.apply(classify_bmi, axis=1)\n",
    "\n",
    "# Remove duplicates from the 'data' DataFrame\n",
    "df = data.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'charges' column and store it in a separate DataFrame\n",
    "y = df[['charges']]\n",
    "\n",
    "# Drop the 'charges' column from the 'data' DataFrame and store the rest of the columns in a separate DataFrame\n",
    "X = df.drop(columns=['charges'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, train_size=0.8, random_state=42, stratify=X[['smoker']])\n",
    "# shuffle: This is a boolean parameter that determines whether the data should be shuffled before splitting. If True, the data will be shuffled randomly before the split. If False, the data will be split in the order it is in the DataFrame.\n",
    "# train_size: This is a float parameter that determines the proportion of the data that should be included in the training set. For example, if train_size=0.8, 80% of the data will be included in the training set and the remaining 20% will be included in the test set.\n",
    "# random_state: This is an optional integer parameter that sets the random seed for shuffling the data. This can be useful for reproducibility of the split.\n",
    "\n",
    "numerical_features = make_column_selector(dtype_include=np.number)\n",
    "categorical_features = make_column_selector(dtype_exclude= np.number)\n",
    "\n",
    "# Create a preprocessing pipeline for numerical features\n",
    "numerical_pipeline = make_pipeline(StandardScaler())\n",
    "\n",
    "# Create a preprocessing pipeline for categorical features\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown='ignore'))\n",
    "# The handle_unknown parameter of the OneHotEncoder transformer in scikit-learn is used to specify how the transformer should handle categorical levels (i.e., categories) that are present in the test data but not in the training data.\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, train_size=0.8, random_state=42, stratify=X[['smoker']])\n",
    "numerical_features = make_column_selector(dtype_include=np.number)\n",
    "categorical_features = make_column_selector(dtype_exclude= np.number)\n",
    "numerical_pipeline = make_pipeline(StandardScaler(with_mean=False))\n",
    "categorical_pipeline = make_pipeline(OneHotEncoder(handle_unknown='ignore'))\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),\n",
    "                                (categorical_pipeline, categorical_features)\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR without Cross Validation\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8190058280369791"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"LR without Cross Validation\")\n",
    "print(\"=\"*50)\n",
    "model = make_pipeline(preprocessor, LinearRegression())\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR with Cross Validation\n",
      "==================================================\n",
      "\n",
      "[0.69554494 0.77739721 0.72434149 0.75347026 0.763884   0.69695614\n",
      " 0.7831847  0.77362682 0.7016041  0.6123441 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8190058280369791"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"LR with Cross Validation\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "model = make_pipeline(preprocessor, LinearRegression())\n",
    "scores = cross_val_score(model, X_train, y_train, cv=10)\n",
    "print(scores)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR with Kfold Cross Validation\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8190058280369791"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"LR with Kfold Cross Validation\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "model = make_pipeline(preprocessor, LinearRegression())\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=kfold)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO\n",
      "==================================================\n",
      "\n",
      "Lasso best alpha =  75.6781081081081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8202293518262205"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"LASSO\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "#model = make_pipeline(preprocessor, Lasso(max_iter=100000, tol=0.0001,random_state=42, selection='cyclic'))\n",
    "#param_grid = {'lasso__alpha': np.linspace(0.01,100,1000)}\n",
    "#grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "#grid_search.fit(X_train, y_train)\n",
    "#best_params = grid_search.best_params_\n",
    "#print(best_params)\n",
    "#model = grid_search.best_estimator_\n",
    "print('Lasso best alpha =  75.6781081081081')\n",
    "best={'lasso__alpha': 75.6781081081081}\n",
    "model = make_pipeline(preprocessor, Lasso(alpha=best[\"lasso__alpha\"],max_iter=100000,random_state=42))\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO with PolynomialFeatures(degree=2)\n",
      "==================================================\n",
      "\n",
      "Lasso best alpha =  43.84945945945945\n",
      "time to find best alpha : 10min 27.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9226943483692917"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"LASSO with PolynomialFeatures(degree=2)\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "# model = make_pipeline(preprocessor, PolynomialFeatures(degree=2), Lasso(max_iter=100000, tol=0.0001,random_state=42, selection='cyclic'))\n",
    "#param_grid = {'lasso__alpha': np.linspace(0.01,100,1000)}\n",
    "#grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "#grid_search.fit(X_train, y_train)\n",
    "#best_params = grid_search.best_params_\n",
    "#print(best_params)\n",
    "#model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "print('Lasso best alpha =  43.84945945945945')\n",
    "print(\"time to find best alpha : 10min 27.6s\")\n",
    "best={'lasso__alpha': 43.84945945945945}\n",
    "model = make_pipeline(preprocessor, PolynomialFeatures(degree=2), Lasso(alpha=best[\"lasso__alpha\"],max_iter=100000,random_state=42))\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "728166df596dc6b9d95aa00a68d9d66253e0ab53dc6b17d60ab3234c59362003"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
