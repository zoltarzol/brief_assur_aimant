When splitting a dataset 80 % / 20% for a machine learning process, what is data leaking?

Data leaking is when information from the test set is used to create or tune the model, resulting in artificially inflated scores on the test set. This is a form of cheating and can lead to overfitting and poor generalization performance.

How to avoid it?

To avoid data leaking, make sure to split the data first, before doing any preprocessing or feature engineering. Additionally, keep the test set completely separate from any process that could influence its results, such as data exploration or model training. Finally, if possible, use cross-validation techniques to further reduce the risk of data leakage.

=========

When it comes to machine learning process, what is a dummy model?

A dummy model is a machine learning model that is used to make sure that any changes made to the data or algorithms being used do not impact the performance of the model. It is essentially a baseline or benchmark model with no predictive capability, used to test changes before they are applied to a fully trained model.

I don't understand, provide an example in Python.

For example, say we are training a machine learning model to predict a particular outcome. We could create a dummy model that uses the same input features we will use for the final model, but with random weights initialized for each feature. We can then use this dummy model as a baseline for comparing performance before and after changes to the data or algorithms being used. That way, we can make sure that any changes we make do not negatively impact the performance of the final model. 

In Python, a dummy model could be created using scikit-learn's Dummy Classifier like this: 

from sklearn.dummy import DummyClassifier

dummy_model = DummyClassifier(strategy="uniform", random_state=42)

dummy_model.fit(X_train, y_train)

y_pred = dummy_model.predict(X_test)

=========

Using Python and Sklearn, please provide commented code illustrating linear regression, Lasso, Ridge and ElasticNet.

#Linear Regression

from sklearn.linear_model import LinearRegression
X = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]
y = [4, 5, 20, 14, 32, 22, 38, 43]
X_test = [[20, 2], [40, 4]]

# Create and fit the linear regression model
model = LinearRegression()
model.fit(X, y)

# Make predictions for the test data
predictions = model.predict(X_test)
print(predictions)

#Lasso 

from sklearn.linear_model import Lasso
X = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]
y = [4, 5, 20, 14, 32, 22, 38, 43]
X_test = [[20, 2], [40, 4]]

# Create and fit the Lasso model 
model = Lasso(alpha=0.1) 
model.fit(X, y)

# Make predictions for the test data 
predictions = model.predict(X_test) 
print(predictions) 

#Ridge 

from sklearn.linear_model import Ridge 
X = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]] 
y = [4, 5, 20, 14, 32, 22, 38, 43] 
X_test = [[20, 2], [40, 4]] 

# Create and fit the Ridge model 
model = Ridge(alpha=0.1) 
model.fit(X, y) 

# Make predictions for the test data 
predictions = model.predict(X_test) 
print(predictions) 


#ElasticNet 

from sklearn.linear_model import ElasticNet 
X = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]] 
y = [4, 5, 20, 14, 32, 22, 38, 43] 
X_test = [[20, 2], [40, 4]] 

# Create and fit the ElasticNet model 
model = ElasticNet(alpha=0.1) 
model.fit(X, y) 

# Make predictions for the test data 
predictions = model.predict(X_test) 
print(predictions)

=========

For each of the following algorithms, please explain the differences, the pertinent use cases and why use one VS another. For information, I'm using Python and sklearn.

- linear regression
- Lasso
- Ridge
- ElasticNet


Linear Regression: Linear Regression is a supervised machine learning algorithm that predicts a continuous numerical value from a set of features. It models the relationship between a dependent variable (target variable) and independent variables (features) by fitting a linear equation to observed data. Linear Regression is the most basic form of regression and is used to predict a response variable given one or more predictor variables. It is used for predicting trends in data and predicting future values.

Lasso: Lasso (Least Absolute Shrinkage and Selection Operator) is a regularization technique which works by penalizing the complexity of the model, that is, it introduces a penalty term to make complex models simpler while still keeping predictive power. It is an extension of linear regression that adds a penalty term to the loss function to reduce the complexity of the model and make it more interpretable. In comparison with linear regression, Lasso tends to produce sparser models that are easier to interpret than those generated by linear regression.

Ridge: Ridge is another regularization technique which works by adding a penalty term to the loss function to reduce the complexity of the model. It works by shrinking the parameter values towards zero but not eliminating them completely, as opposed to lasso which sets some parameter values to zero. The main advantage of ridge regression over lasso is that it does not eliminate any features from the model, so it can be used when all features are important.

ElasticNet: ElasticNet is a combination of both Lasso and Ridge regularization techniques. It combines their penalties together by adding both an L1 (Lasso) and an L2 (Ridge) penalty term to the loss function. The elastic net technique allows for more flexibility in the model as it allows for both feature elimination and parameter shrinkage. This makes it more suitable for scenarios where some variables are more important than others and can be used in cases where some variables may be correlated.